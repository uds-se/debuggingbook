{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Statistical Debugging\n",
    "\n",
    "In this chapter, we introduce _statistical debugging_ – the idea that specific events during execution could be _statistically correlated_ with failures. We start with coverage of individual lines and then proceed towards further execution features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bookutils import YouTubeVideo\n",
    "YouTubeVideo(\"UNuso00zYiI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Prerequisites**\n",
    "\n",
    "* You should have read the [chapter on tracing executions](Tracer.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import bookutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Synopsis\n",
    "<!-- Automatically generated. Do not edit. -->\n",
    "\n",
    "To [use the code provided in this chapter](Importing.ipynb), write\n",
    "\n",
    "```python\n",
    ">>> from debuggingbook.StatisticalDebugger import <identifier>\n",
    "```\n",
    "\n",
    "and then make use of the following features.\n",
    "\n",
    "\n",
    "This chapter introduces classes and techniques for _statistical debugging_ – that is, correlating specific events, such as lines covered, with passing and failing outcomes.\n",
    "\n",
    "To make use of the code in this chapter, use one of the provided `StatisticalDebugger` subclasses such as `TarantulaDebugger` or `OchiaiDebugger` instantiated with a `Collector` such as `CoverageCollector` denoting the type of events you want to correlate outcomes with. Here's a typical example:\n",
    "\n",
    "```python\n",
    ">>> debugger = TarantulaDebugger(CoverageCollector)\n",
    ">>> with debugger.collect_pass():\n",
    ">>>     remove_html_markup(\"abc\")\n",
    ">>> with debugger.collect_pass():\n",
    ">>>     remove_html_markup('<b>abc</b>')\n",
    ">>> with debugger.collect_fail():\n",
    ">>>     remove_html_markup('\"abc\"')\n",
    "```\n",
    "You can then print out the observed events – in this case, line numbers – in a table, showing in which runs they occurred (`X`), and with colors highlighting the suspiciousness of the event. A \"red\" event means that the event predominantly occurs in failing runs.\n",
    "\n",
    "```python\n",
    ">>> debugger.event_table(args=True, color=True)\n",
    "```\n",
    "| `remove_html_markup` | `s='abc'` | `s='<b>abc</b>'` | `s='\"abc\"'` | \n",
    "| -- | ---- | ---- | ---- | \n",
    "| <samp style=\"background-color: hsl(60.0, 100.0%, 80%)\"> 1</samp> |    X |    X |    X | \n",
    "| <samp style=\"background-color: hsl(60.0, 100.0%, 80%)\"> 2</samp> |    X |    X |    X | \n",
    "| <samp style=\"background-color: hsl(60.0, 100.0%, 80%)\"> 3</samp> |    X |    X |    X | \n",
    "| <samp style=\"background-color: hsl(60.0, 100.0%, 80%)\"> 4</samp> |    X |    X |    X | \n",
    "| <samp style=\"background-color: hsl(60.0, 100.0%, 80%)\"> 6</samp> |    X |    X |    X | \n",
    "| <samp style=\"background-color: hsl(60.0, 100.0%, 80%)\"> 7</samp> |    X |    X |    X | \n",
    "| <samp style=\"background-color: hsl(120.0, 50.0%, 80%)\"> 8</samp> |    - |    X |    - | \n",
    "| <samp style=\"background-color: hsl(60.0, 100.0%, 80%)\"> 9</samp> |    X |    X |    X | \n",
    "| <samp style=\"background-color: hsl(120.0, 50.0%, 80%)\">10</samp> |    - |    X |    - | \n",
    "| <samp style=\"background-color: hsl(60.0, 100.0%, 80%)\">11</samp> |    X |    X |    X | \n",
    "| <samp style=\"background-color: hsl(0.0, 100.0%, 80%)\">12</samp> |    - |    - |    X | \n",
    "| <samp style=\"background-color: hsl(60.0, 100.0%, 80%)\">13</samp> |    X |    X |    X | \n",
    "| <samp style=\"background-color: hsl(60.0, 100.0%, 80%)\">14</samp> |    X |    X |    X | \n",
    "| <samp style=\"background-color: hsl(60.0, 100.0%, 80%)\">16</samp> |    X |    X |    X | \n",
    "\n",
    "If you collected coverage with `CoverageCollector`, you can also visualize the code with similar colors, highlighting suspicious lines:\n",
    "\n",
    "```python\n",
    ">>> debugger.list_with_spectrum()\n",
    "```\n",
    "<pre style=\"background-color:hsl(60.0, 100.0%, 80%)\">   1 def remove_html_markup(s):</pre>\n",
    "<pre style=\"background-color:hsl(60.0, 100.0%, 80%)\">   2     tag = False</pre>\n",
    "<pre style=\"background-color:hsl(60.0, 100.0%, 80%)\">   3     quote = False</pre>\n",
    "<pre style=\"background-color:hsl(60.0, 100.0%, 80%)\">   4     out = &quot;&quot;</pre>\n",
    "<pre>   5 &nbsp;</pre>\n",
    "<pre style=\"background-color:hsl(60.0, 100.0%, 80%)\">   6     for c in s:</pre>\n",
    "<pre style=\"background-color:hsl(60.0, 100.0%, 80%)\">   7         if c == &#x27;&lt;&#x27; and not quote:</pre>\n",
    "<pre style=\"background-color:hsl(120.0, 50.0%, 80%)\">   8             tag = True</pre>\n",
    "<pre style=\"background-color:hsl(60.0, 100.0%, 80%)\">   9         elif c == &#x27;&gt;&#x27; and not quote:</pre>\n",
    "<pre style=\"background-color:hsl(120.0, 50.0%, 80%)\">  10             tag = False</pre>\n",
    "<pre style=\"background-color:hsl(60.0, 100.0%, 80%)\">  11         elif c == &#x27;&quot;&#x27; or c == &quot;&#x27;&quot; and tag:</pre>\n",
    "<pre style=\"background-color:hsl(0.0, 100.0%, 80%)\">  12             quote = not quote</pre>\n",
    "<pre style=\"background-color:hsl(60.0, 100.0%, 80%)\">  13         elif not tag:</pre>\n",
    "<pre style=\"background-color:hsl(60.0, 100.0%, 80%)\">  14             out = out + c</pre>\n",
    "<pre>  15 &nbsp;</pre>\n",
    "<pre style=\"background-color:hsl(60.0, 100.0%, 80%)\">  16     return out</pre>\n",
    "\n",
    "The method `rank_by_suspiciousness()` returns a ranked list of events, starting with the most suspicious. This is useful for automated techniques that need potential defect locations.\n",
    "\n",
    "```python\n",
    ">>> debugger.rank_by_suspiciousness()\n",
    "[12, 1, 2, 3, 4, 6, 7, 9, 11, 13, 14, 16, 8, 10]\n",
    "```\n",
    "Here are all classes defined in this chapter:\n",
    "\n",
    "```python\n",
    "```\n",
    "![](PICS/StatisticalDebugger-synopsis-1.svg)\n",
    "![](PICS/StatisticalDebugger-synopsis-2.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The idea behind _statistical debugging_ is fairly simple. We have a program that sometimes passes and sometimes fails. This outcome can be _correlated_ with events that precede it – properties of the input, properties of the execution, properties of the program state. If we, for instance, can find that \"the program always fails when Line 123 is executed, and it always passes when Line 123 is _not_ executed\", then we have a strong correlation between Line 123 being executed and failure.\n",
    "\n",
    "Such _correlation_ does not necessarily mean _causation_. For this, we would have to prove that executing Line 123 _always_ leads to failure, and that _not_ executing it does not lead to (this) failure. Also, a correlation (or even a causation) does not mean that Line 123 contains the defect – for this, we would have to show that it actually is an error. Still, correlations make excellent hints as it comes to search for failure causes – in all generality, if you let your search be guided by _events that correlate with failures_, you are more likely to find _important hints on how the failure comes to be_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Collecting Events\n",
    "\n",
    "How can we determine events that correlate with failure? We start with a general mechanism to actually _collect_ events during execution. The abstract `Collector` class provides\n",
    "\n",
    "* a `collect()` method made for collecting events, called from the `traceit()` tracer; and\n",
    "* an `events()` method made for retrieving these events.\n",
    "\n",
    "Both of these are _abstract_ and will be defined further in subclasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tracer import Tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collector(Tracer):\n",
    "    \"\"\"A class to record events during execution.\"\"\"\n",
    "\n",
    "    def collect(self, frame, event, arg):\n",
    "        \"\"\"Collecting function. To be overridden in subclasses.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def events(self):\n",
    "        \"\"\"Return a collection of events. To be overridden in subclasses.\"\"\"\n",
    "        return set()\n",
    "\n",
    "    def traceit(self, frame, event, arg):\n",
    "        self.collect(frame, event, arg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `Collector` class is used like `Tracer`, using a `with` statement. Let us apply it on the buggy variant of `remove_html_markup()` from the [Introduction to Debugging](Intro_Debugging.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_markup(s):\n",
    "    tag = False\n",
    "    quote = False\n",
    "    out = \"\"\n",
    "\n",
    "    for c in s:\n",
    "        if c == '<' and not quote:\n",
    "            tag = True\n",
    "        elif c == '>' and not quote:\n",
    "            tag = False\n",
    "        elif c == '\"' or c == \"'\" and tag:\n",
    "            quote = not quote\n",
    "        elif not tag:\n",
    "            out = out + c\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Collector() as c:\n",
    "    out = remove_html_markup('\"abc\"')\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's not much we can do with our collector, as the `collect()` and `events()` methods are yet empty. However, we can introduce an `id()` method which returns a string identifying the collector. This string is defined from the _first function call_ encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import FunctionType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collector(Collector):\n",
    "    def __init__(self):\n",
    "        self._function = None\n",
    "        self._args = None\n",
    "\n",
    "    def traceit(self, frame, event, arg):\n",
    "        if self._function is None and event == 'call':\n",
    "            # Save function\n",
    "            self._function = FunctionType(frame.f_code,\n",
    "                                          globals=globals(),\n",
    "                                          name=frame.f_code.co_name)\n",
    "            locals = frame.f_locals\n",
    "            self._args = \", \".join([f\"{var}={repr(locals[var])}\" for var in locals])\n",
    "\n",
    "        self.collect(frame, event, arg)\n",
    "\n",
    "    def id(self):\n",
    "        \"\"\"Return an identifier for the collector, created from the first call\"\"\"\n",
    "        return f\"{self._function.__name__}({self._args})\"\n",
    "\n",
    "    def function(self):\n",
    "        \"\"\"Return the function from the first call, as a function object\"\"\"\n",
    "        return self._function\n",
    "\n",
    "    def args(self):\n",
    "        \"\"\"Return the list of arguments from the first call, as a string\"\"\"\n",
    "        return self._args\n",
    "\n",
    "    def __repr__(self):\n",
    "        # We use the ID as default representation when printed\n",
    "        return self.id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Collector() as c:\n",
    "    remove_html_markup('abc')\n",
    "c.function(), c.id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting Coverage\n",
    "\n",
    "So far, our `Collector` class does not collect any events. Let us extend it such that it collects _coverage_ information – that is, the set of lines executed. To this end, we introduce a `CoverageCollector` subclass which saves the coverage in a set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoverageCollector(Collector):\n",
    "    \"\"\"A class to record covered lines during execution.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.coverage = set()\n",
    "\n",
    "    def collect(self, frame, event, arg):\n",
    "        self.coverage.add(frame.f_lineno)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also override `events()` such that it returns the set of covered lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoverageCollector(CoverageCollector):\n",
    "    def events(self):\n",
    "        \"\"\"Return a set of predicates holding for the execution\"\"\"\n",
    "        return self.coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we can use `CoverageCollector` to determine the lines executed during a run of `remove_html_markup()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with CoverageCollector() as c:\n",
    "    remove_html_markup('abc')\n",
    "c.events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets of line numbers alone are not too revealing. They provide more insights if we actually list the code, highlighting these numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bookutils import getsourcelines    # like inspect.getsourcelines(), but in color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_with_coverage(function, coverage):\n",
    "    source_lines, starting_line_number = \\\n",
    "       getsourcelines(function)\n",
    "\n",
    "    line_number = starting_line_number\n",
    "    for line in source_lines:\n",
    "        marker = '*' if line_number in coverage else ' '\n",
    "        print(f\"{line_number:4} {marker} {line}\", end='')\n",
    "        line_number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_with_coverage(remove_html_markup, c.coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the input `s` was `\"abc\"`? In this listing, we can see which lines were covered and which lines were not. From the listing already, we can see that `s` has neither tags nor quotes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such coverage computation plays a big role in _testing_, as one wants tests to cover as many different aspects of program execution (and notably code) as possible. But also during debugging, code coverage is essential: If some code was not even executed in the failing run, then any change to it will have no effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bookutils import quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz('Let the input be `\"<b>Don\\'t do this!</b>\"`. '\n",
    "     \"Which of these lines are executed? Use the code to find out!\",\n",
    "     [\n",
    "         \"`tag = True`\",\n",
    "         \"`tag = False`\",\n",
    "         \"`quote = not quote`\",\n",
    "         \"`out = out + c`\"\n",
    "     ], [ord(c) - ord('a') - 1 for c in 'cdf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the solution, try this out yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with CoverageCollector() as c:\n",
    "    remove_html_markup(\"<b>Don't do this!</b>\")\n",
    "# list_with_coverage(remove_html_markup, c.coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Differences\n",
    "\n",
    "Let us get back to the idea that we want to _correlate_ events with passing and failing outcomes. For this, we need to examine events in both _passing_ and _failing_ runs, and determine their _differences_ – since it is these differences we want to associate with their respective outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Base Class for Statistical Debugging\n",
    "\n",
    "The `StatisticalDebugger` base class takes a collector class (such as `CoverageCollector`). Its `collect()` method creates a new collector of that very class, which will be maintained by the debugger. As argument, `collect()` takes a string characterizing the outcome (such as `'PASS'` or `'FAIL'`). This is how one would use it:\n",
    "\n",
    "```python\n",
    "debugger = StatisticalDebugger(CoverageCollector)\n",
    "with debugger.collect('PASS'):\n",
    "    some_passing_run()\n",
    "with debugger.collect('PASS'):\n",
    "    another_passing_run()\n",
    "with debugger.collect('FAIL'):\n",
    "    some_failing_run()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us implement `StatisticalDebugger`. The base class gets a collector class as argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticalDebugger():\n",
    "    \"\"\"A class to collect events for multiple outcomes.\"\"\"\n",
    "\n",
    "    def __init__(self, collector_class):\n",
    "        self.collector_class = collector_class\n",
    "        self.collectors = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `collect()` method creates (and stores) a collector for the given outcome, using the given outcome to characterize the run. Any additional arguments are passed to the collector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticalDebugger(StatisticalDebugger):\n",
    "    def collect(self, outcome, *args):\n",
    "        \"\"\"Return a collector for the given outcome. \n",
    "        Additional args are passed to the collector.\"\"\"\n",
    "        collector = self.collector_class(*args)\n",
    "        if outcome not in self.collectors:\n",
    "            self.collectors[outcome] = []\n",
    "        self.collectors[outcome].append(collector)\n",
    "        return collector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `all_events()` method produces a union of all events observed. If an outcome is given, it produces a union of all events with that outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticalDebugger(StatisticalDebugger):\n",
    "    def all_events(self, outcome=None):\n",
    "        \"\"\"Return a set of all events observed.\"\"\"\n",
    "        all_events = set()\n",
    "        if outcome:\n",
    "            for collector in self.collectors[outcome]:\n",
    "                all_events.update(collector.events())\n",
    "        else:\n",
    "            for outcome in self.collectors:\n",
    "                for collector in self.collectors[outcome]:\n",
    "                    all_events.update(collector.events())\n",
    "        return all_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple example of `StatisticalDebugger` in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = StatisticalDebugger(CoverageCollector)\n",
    "with s.collect('PASS'):\n",
    "    remove_html_markup(\"abc\")\n",
    "with s.collect('PASS'):\n",
    "    remove_html_markup('<b>abc</b>')\n",
    "with s.collect('FAIL'):\n",
    "    remove_html_markup('\"abc\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `all_events()` returns all events collected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.all_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If given an outcome as argument, we obtain all events with the given outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.all_events('FAIL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute `collectors` maps outcomes to lists of collectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.collectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the collector of the one (and first) passing run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.collectors['PASS'][0].id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.collectors['PASS'][0].events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better highlight the differences between the collected events, we introduce a method `event_table()` that prints out whether an event took place in a run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excursion: Printing an Event Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatisticalDebugger(StatisticalDebugger):\n",
    "    def function(self):\n",
    "        \"\"\"Return the function from the events observed, or None if ambiguous\"\"\"\n",
    "        function = None\n",
    "        for outcome in self.collectors:\n",
    "            for collector in self.collectors[outcome]:\n",
    "                if function is None:\n",
    "                    function = collector.function()\n",
    "                elif function.__name__ != collector.function().__name__:\n",
    "                    return None  # ambiguous\n",
    "\n",
    "        return function\n",
    "\n",
    "    def color(self, event):\n",
    "        \"\"\"Return a color for the given event, or None. To be overloaded in subclasses.\"\"\"\n",
    "        return None\n",
    "\n",
    "    def event_table_text(self, args=False, color=False):\n",
    "        \"\"\"Print out a table of events observed.\n",
    "           If args is set, use arguments as headers.\n",
    "           If color is set, use colors.\"\"\"\n",
    "        sep = ' | '\n",
    "\n",
    "        all_events = self.all_events()\n",
    "        longest_event = max(len(f\"{event}\") for event in all_events)\n",
    "\n",
    "        out = \"\"\n",
    "\n",
    "        # Header\n",
    "        if args:\n",
    "            out += '| ' + '`' + self.function().__name__ + '`' + sep\n",
    "            for name in self.collectors:\n",
    "                for collector in self.collectors[name]:\n",
    "                    out += '`' + collector.args() + '`' + sep\n",
    "            out += '\\n'\n",
    "        else:\n",
    "            out += '| ' + ' ' * longest_event + sep\n",
    "            for name in self.collectors:\n",
    "                for i in range(len(self.collectors[name])):\n",
    "                    out += name + sep\n",
    "            out += '\\n'\n",
    "\n",
    "        out += '| ' + '-' * longest_event + sep\n",
    "        for name in self.collectors:\n",
    "            for i in range(len(self.collectors[name])):\n",
    "                out += '-' * len(name) + sep\n",
    "        out += '\\n'\n",
    "\n",
    "        # Data\n",
    "        for event in all_events:\n",
    "            event_name = str(event).rjust(longest_event)  # could also use repr(event)\n",
    "\n",
    "            if color:\n",
    "                color_name = self.color(event)\n",
    "                if color_name:\n",
    "                    event_name = \\\n",
    "                        f'<samp style=\"background-color: {color_name}\">{html.escape(event_name)}</samp>'\n",
    "\n",
    "            out += f\"| {event_name}\" + sep\n",
    "            for name in self.collectors:\n",
    "                for collector in self.collectors[name]:\n",
    "                    out += ' ' * (len(name) - 1)\n",
    "                    if event in collector.events():\n",
    "                        out += \"X\"\n",
    "                    else:\n",
    "                        out += \"-\"\n",
    "                    out += sep\n",
    "            out += '\\n'\n",
    "\n",
    "        return out\n",
    "\n",
    "    def event_table(self, **_args):\n",
    "        \"\"\"Print out event table in Markdown format.\"\"\"\n",
    "        return Markdown(self.event_table_text(**_args))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._event_table_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Excursion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = StatisticalDebugger(CoverageCollector)\n",
    "with s.collect('PASS'):\n",
    "    remove_html_markup(\"abc\")\n",
    "with s.collect('PASS'):\n",
    "    remove_html_markup('<b>abc</b>')\n",
    "with s.collect('FAIL'):\n",
    "    remove_html_markup('\"abc\"')\n",
    "s.event_table(args=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz(\"How many lines are executed in the failing run only?\",\n",
    "    [\"One\", \"Two\", \"Three\"], int(chr(50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These lines only executed in the failing run would be a correlation to look for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting Passing and Failing Runs\n",
    "\n",
    "While our `StatisticalDebugger` class allows arbitrary outcomes, we are typically only interested in two outcomes, namely _passing_ vs. _failing_ runs. We therefore introduce a specialized `DifferenceDebugger` class that provides customized methods to collect and access passing and failing runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferenceDebugger(StatisticalDebugger):\n",
    "    \"\"\"A class to collect events for passing and failing outcomes.\"\"\"\n",
    "\n",
    "    PASS = 'PASS'\n",
    "    FAIL = 'FAIL'\n",
    "\n",
    "    def collect_pass(self, *args):\n",
    "        \"\"\"Return a collector for passing runs.\"\"\"\n",
    "        return self.collect(self.PASS, *args)\n",
    "\n",
    "    def collect_fail(self, *args):\n",
    "        \"\"\"Return a collector for failing runs.\"\"\"\n",
    "        return self.collect(self.FAIL, *args)\n",
    "\n",
    "    def pass_collectors(self):\n",
    "        return self.collectors[self.PASS]\n",
    "\n",
    "    def fail_collectors(self):\n",
    "        return self.collectors[self.FAIL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to use `DifferenceDebugger`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_debugger_html(debugger):\n",
    "    with debugger.collect_pass():\n",
    "        remove_html_markup('abc')\n",
    "    with debugger.collect_pass():\n",
    "        remove_html_markup('<b>abc</b>')\n",
    "    with debugger.collect_fail():\n",
    "        remove_html_markup('\"abc\"')\n",
    "    return debugger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(DifferenceDebugger(CoverageCollector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since events come back as _sets_, we can compute _unions_ and _differences_ between these sets. For instance, we can compute which lines were executed in _any_ of the passing runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_1_events = debugger.pass_collectors()[0].events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_2_events = debugger.pass_collectors()[1].events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_any_pass = pass_1_events | pass_2_events\n",
    "in_any_pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we can determine which lines were _only_ executed in the failing run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_events = debugger.fail_collectors()[0].events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "only_in_fail = fail_events - in_any_pass\n",
    "only_in_fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that the \"failing\" run is characterized by processing quotes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_with_coverage(remove_html_markup, only_in_fail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us add a few helper methods that return computations such as the above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferenceDebugger(DifferenceDebugger):\n",
    "    def all_fail_events(self):\n",
    "        \"\"\"Return all events observed in failing runs.\"\"\"\n",
    "        return self.all_events(self.FAIL)\n",
    "\n",
    "    def all_pass_events(self):\n",
    "        \"\"\"Return all events observed in passing runs.\"\"\"\n",
    "        return self.all_events(self.PASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now introduce helper methods that show the events occurring only in passing and failing runs, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DifferenceDebugger(DifferenceDebugger):\n",
    "    def only_fail_events(self):\n",
    "        \"\"\"Return all events observed only in failing runs.\"\"\"\n",
    "        return self.all_fail_events() - self.all_pass_events()\n",
    "\n",
    "    def only_pass_events(self):\n",
    "        \"\"\"Return all events observed only in passing runs.\"\"\"\n",
    "        return self.all_pass_events() - self.all_fail_events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(DifferenceDebugger(CoverageCollector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.all_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the lines executed only in the failing run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.only_fail_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the lines executed only in the passing runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.only_pass_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, having these lines individually is neat, but things become much more interesting if we can see the associated code lines just as well. That's what we will do in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Differences\n",
    "\n",
    "To show correlations of line coverage in context, we introduce a number of _visualization_ techniques that _highlight_ code with different colors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Spectrum\n",
    "\n",
    "The first idea is to use a _discrete_ spectrum of three colors:\n",
    "\n",
    "* _red_ for code executed in failing runs only\n",
    "* _green_ for code executed in passing runs only\n",
    "* _yellow_ for code executed in both passing and failing runs.\n",
    "\n",
    "Code that is not executed stays unhighlighted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `DiscreteSpectrumDebugger` subclass provides a `color()` method that returns one of these three colors depending on the line number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteSpectrumDebugger(DifferenceDebugger):\n",
    "    def suspiciousness(self, event):\n",
    "        \"\"\"Return a suspiciousness value [0, 1.0] for the given event, or None if unknown\"\"\"\n",
    "        passing = self.all_pass_events()\n",
    "        failing = self.all_fail_events()\n",
    "\n",
    "        if event in passing and event in failing:\n",
    "            return 0.5\n",
    "        elif event in failing:\n",
    "            return 1.0\n",
    "        elif event in passing:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def color(self, event):\n",
    "        \"\"\"Return a color for the given event.\"\"\"\n",
    "        suspiciousness = self.suspiciousness(event)\n",
    "        if suspiciousness is None:\n",
    "            return None\n",
    "\n",
    "        if suspiciousness > 0.8:\n",
    "            return 'mistyrose'\n",
    "        if suspiciousness >= 0.5:\n",
    "            return 'lightyellow'\n",
    "        \n",
    "        return 'honeydew'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `list_with_spectrum()` method takes a function and shows each of its source code lines using the given spectrum, using HTML markup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteSpectrumDebugger(DiscreteSpectrumDebugger):\n",
    "    def list_with_spectrum(self, function=None, show_color_names=False):\n",
    "        \"\"\"Print a listing of the given function, using suspiciousness colors.\"\"\"\n",
    "        if function is None:\n",
    "            function = self.function()\n",
    "        if function is None:\n",
    "            raise ValueError(\"Must specify function to list\")\n",
    "\n",
    "        source_lines, starting_line_number = \\\n",
    "           inspect.getsourcelines(function)\n",
    "\n",
    "        line_number = starting_line_number\n",
    "        out = \"\"\n",
    "        for line in source_lines:\n",
    "            line = html.escape(line)\n",
    "            if line.strip() == '':\n",
    "                line = '&nbsp;'\n",
    "\n",
    "            line = str(line_number).rjust(4) + ' ' + line\n",
    "            color = self.color(line_number)\n",
    "\n",
    "            if show_color_names:\n",
    "                line = f'{repr(color):20} {line}'\n",
    "\n",
    "            if color:\n",
    "                line = f'<pre style=\"background-color:{color}\">' \\\n",
    "                        f'{line.rstrip()}</pre>'\n",
    "            else:\n",
    "                line = f'<pre>{line}</pre>'\n",
    "\n",
    "            out += line + '\\n'\n",
    "            line_number += 1\n",
    "\n",
    "        return HTML(out)\n",
    "\n",
    "    def list_with_suspiciousness(self, function=None):\n",
    "        \"\"\"Print a listing of the given function, using suspiciousness values.\"\"\"\n",
    "        if function is None:\n",
    "            function = self.function()\n",
    "        if function is None:\n",
    "            raise ValueError(\"Must specify function to list\")\n",
    "\n",
    "        source_lines, starting_line_number = \\\n",
    "           inspect.getsourcelines(function)\n",
    "\n",
    "        line_number = starting_line_number\n",
    "        out = \"\"\n",
    "        for line in source_lines:\n",
    "            suspiciousness = self.suspiciousness(line_number)\n",
    "            line = str(line_number).rjust(4) + ' ' + str(suspiciousness).rjust(4) + line\n",
    "            out += line + '\\n'\n",
    "            line_number += 1\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.list_with_suspiciousness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the `only_pass_events()` and `only_fail_events()` sets look like when visualized with code. The \"culprit\" line is well highlighted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(DiscreteSpectrumDebugger(CoverageCollector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.list_with_spectrum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the failure is correlated with the presence of quotes in the input string (which is an important hint!). But does this also show us _immediately_ where the defect to be fixed is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz(\"Does the line `quote = not quote` actually contain the defect?\",\n",
    "    [\n",
    "        \"Yes, it should be fixed\",\n",
    "        \"No, the defect is elsewhere\"\n",
    "    ],\n",
    "     164 * 2 % 326\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, it is the preceding condition that is wrong. In order to fix a program, we have to find a location that\n",
    "\n",
    "1. _causes_ the failure (i.e., it can be changed to make the failure go away); and\n",
    "2. is a _defect_ (i.e., contains an error).\n",
    "\n",
    "In our example above, the highlighted code line is a _symptom_ for the error. To some extent, it is also a _cause_, since, say, commenting it out would also resolve the given failure, at the cost of causing other failures. However, the preceding condition also is a cause, as is the presence of quotes in the input.\n",
    "\n",
    "Only one of these also is a _defect_, though, and that is the preceding condition. Hence, while correlations can provide important hints, they do not necessarily locate defects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those of us who may not have color HTML output ready, we introduce a variant that simply lists suspiciousness values as percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteSpectrumDebugger(DiscreteSpectrumDebugger):\n",
    "    def list_with_suspiciousness(self, function=None):\n",
    "        \"\"\"Print a listing of the given function, including suspiciousness values.\"\"\"\n",
    "        if function is None:\n",
    "            function = self.function()\n",
    "        if function is None:\n",
    "            raise ValueError(\"Must specify function to list\")\n",
    "\n",
    "        source_lines, starting_line_number = \\\n",
    "           inspect.getsourcelines(function)\n",
    "\n",
    "        line_number = starting_line_number\n",
    "        out = \"\"\n",
    "        for line in source_lines:\n",
    "            suspiciousness = self.suspiciousness(line_number)\n",
    "            if suspiciousness is not None:\n",
    "                percentage = str(int(suspiciousness * 100)).rjust(3) + '%'\n",
    "            else:\n",
    "                percentage = ' ' * len('100%')\n",
    "            line = str(line_number).rjust(4) + ' ' + percentage + ' ' + line\n",
    "            out += line\n",
    "            line_number += 1\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.list_with_suspiciousness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(DiscreteSpectrumDebugger(CoverageCollector))\n",
    "debugger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous Spectrum\n",
    "\n",
    "The criterion that an event should _only_ occur in failing runs (and not in passing runs) can be too aggressive. In particular, if we have another run that executes the \"culprit\" lines, but does _not_ fail, our \"only in fail\" criterion will no longer be helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example. The input\n",
    "\n",
    "```html\n",
    "<b color=\"blue\">text</b>\n",
    "```\n",
    "\n",
    "will trigger the \"culprit\" line\n",
    "\n",
    "```python\n",
    "quote = not quote\n",
    "```\n",
    "\n",
    "but actually produce an output where the tags are properly stripped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_html_markup('<b color=\"blue\">text</b>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a consequence, we no longer have lines that are being executed only in failing runs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(DiscreteSpectrumDebugger(CoverageCollector))\n",
    "with debugger.collect_pass():\n",
    "    remove_html_markup('<b link=\"blue\"></b>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.only_fail_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our spectrum output, the effect now is that the \"culprit\" line is as yellow as all others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.list_with_spectrum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We therefore introduce a different method for highlighting lines, based on their _relative_ occurrence with respect to all runs: If a line has been _mostly_ executed in failing runs, its color should shift towards red; if a line has been _mostly_ executed in passing runs, its color should shift towards green."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This _continuous spectrum_ has been introduced by the seminal _Tarantula_ tool \\cite{Jones2002}. In Tarantula, the color _hue_ for each line is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\textit{color hue}(\\textit{line}) = \\textit{low color(red)} + \\frac{\\%\\textit{passed}(\\textit{line})}{\\%\\textit{passed}(\\textit{line}) + \\%\\textit{failed}(\\textit{line})} \\times \\textit{color range}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, `%passed` and `%failed` denote the percentage at which a line has been executed in passing and failing runs, respectively. A hue of 0.0 stands for red, a hue of 1.0 stands for green, and a hue of 0.5 stands for equal fractions of red and green, yielding yellow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement these measures right away as methods in a new `ContinuousSpectrumDebugger` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousSpectrumDebugger(DiscreteSpectrumDebugger):\n",
    "    def collectors_with_event(self, event, category):\n",
    "        \"\"\"Return all collectors in a category that observed the given event.\"\"\"\n",
    "        all_runs = self.collectors[category]\n",
    "        collectors_with_event = set(collector for collector in all_runs \n",
    "                              if event in collector.events())\n",
    "        return collectors_with_event\n",
    "\n",
    "    def collectors_without_event(self, event, category):\n",
    "        \"\"\"Return all collectors in a category that did not observe the given event.\"\"\"\n",
    "        all_runs = self.collectors[category]\n",
    "        collectors_without_event = set(collector for collector in all_runs \n",
    "                              if event not in collector.events())\n",
    "        return collectors_without_event\n",
    "\n",
    "    def event_fraction(self, event, category):\n",
    "        all_collectors = self.collectors[category]\n",
    "        collectors_with_event = self.collectors_with_event(event, category)\n",
    "        fraction = len(collectors_with_event) / len(all_collectors)\n",
    "        # print(f\"%{category}({event}) = {fraction}\")\n",
    "        return fraction\n",
    "\n",
    "    def passed_fraction(self, line_number):\n",
    "        return self.event_fraction(line_number, self.PASS)\n",
    "\n",
    "    def failed_fraction(self, line_number):\n",
    "        return self.event_fraction(line_number, self.FAIL)\n",
    "\n",
    "    def hue(self, line_number):\n",
    "        \"\"\"Return a color hue from 0.0 (red) to 1.0 (green).\"\"\"\n",
    "        passed = self.passed_fraction(line_number)\n",
    "        failed = self.failed_fraction(line_number)\n",
    "        if passed + failed > 0:\n",
    "            return passed / (passed + failed)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hue for lines executed only in failing runs is (deep) red, as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(ContinuousSpectrumDebugger(CoverageCollector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in debugger.only_fail_events():\n",
    "    print(line, debugger.hue(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, the hue for lines executed in passing runs is (deep) green:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in debugger.only_pass_events():\n",
    "    print(line, debugger.hue(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tarantula tool not only sets the hue for a line, but also uses _brightness_ as measure for support – that is, how often was the line executed at all. The brighter a line, the stronger the correlation with a passing or failing outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The brightness is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\textit{brightness}(line) = \\max(\\%\\textit{passed}(\\textit{line}), \\%\\textit{failed}(\\textit{line}))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and it is easily implemented, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousSpectrumDebugger(ContinuousSpectrumDebugger):\n",
    "    def brightness(self, line):\n",
    "        return max(self.passed_fraction(line), self.failed_fraction(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our single \"only in fail\" line has a brightness of 1.0 (the maximum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(ContinuousSpectrumDebugger(CoverageCollector))\n",
    "for line in debugger.only_fail_events():\n",
    "    print(line, debugger.brightness(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can now define a color for each line. To this end, we override the (previously discrete) `color()` method such that it returns a color specification giving hue and brightness. We use the HTML format `hsl(hue, saturation, lightness)` where the hue is given as a value between 0 and 360 (0 is red, 120 is green) and saturation and lightness are provided as percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousSpectrumDebugger(ContinuousSpectrumDebugger):\n",
    "    def color(self, line):\n",
    "        hue = debugger.hue(line)\n",
    "        if hue is None:\n",
    "            return None\n",
    "        saturation = debugger.brightness(line)\n",
    "\n",
    "        # HSL color values are specified with: \n",
    "        # hsl(hue, saturation, lightness).\n",
    "        return f\"hsl({hue * 120}, {saturation * 100}%, 80%)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(ContinuousSpectrumDebugger(CoverageCollector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lines executed only in failing runs are still shown in red:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in debugger.only_fail_events():\n",
    "    print(line, debugger.color(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... whereas lines executed only in passing runs are still shown in green:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in debugger.only_pass_events():\n",
    "    print(line, debugger.color(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.list_with_spectrum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens with our `quote = not quote` \"culprit\" line if it is executed in passing runs, too?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with debugger.collect_pass():\n",
    "    out = remove_html_markup('<b link=\"blue\"></b>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz('In which color will the `quote = not quote` \"culprit\" line '\n",
    "     'be shown after executing the above code?',\n",
    "    [\n",
    "        '<span style=\"background-color: hsl(120.0, 50.0%, 80%)\">Green</span>',\n",
    "        '<span style=\"background-color: hsl(60.0, 100.0%, 80%)\">Yellow</span>',\n",
    "        '<span style=\"background-color: hsl(30.0, 100.0%, 80%)\">Orange</span>',\n",
    "        '<span style=\"background-color: hsl(0.0, 100.0%, 80%)\">Red</span>'\n",
    "    ],\n",
    "     999 / 333\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it still is shown with an orange-red tint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.list_with_spectrum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's another example, coming right from the Tarantula paper. The `middle()` function takes three numbers `x`, `y`, and `z`, and returns the one that is neither the minimum nor the maximum of the three:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def middle(x, y, z):\n",
    "    if y < z:\n",
    "        if x < y:\n",
    "            return y\n",
    "        elif x < z:\n",
    "            return y\n",
    "    else:\n",
    "        if x > y:\n",
    "            return y\n",
    "        elif x > z:\n",
    "            return x\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle(1, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, `middle()` can fail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle(2, 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let is see whether we can find the bug with a few additional test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_debugger_middle(debugger):\n",
    "    with debugger.collect_pass():\n",
    "        middle(3, 3, 5)\n",
    "    with debugger.collect_pass():\n",
    "        middle(1, 2, 3)\n",
    "    with debugger.collect_pass():\n",
    "        middle(3, 2, 1)\n",
    "    with debugger.collect_pass():\n",
    "        middle(5, 5, 5)\n",
    "    with debugger.collect_pass():\n",
    "        middle(5, 3, 4)\n",
    "    with debugger.collect_fail():\n",
    "        middle(2, 1, 3)\n",
    "    return debugger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in order to collect data from multiple function invocations, you need to have a separate `with` clause for every invocation. The following will _not_ work correctly:\n",
    "\n",
    "```python\n",
    "    with debugger.collect_pass():\n",
    "        middle(3, 3, 5)\n",
    "        middle(1, 2, 3)\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_middle(ContinuousSpectrumDebugger(CoverageCollector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.event_table(args=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here us the visualization. We see that the `return y` line is the culprit here – and actually also the one to be fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.list_with_spectrum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz(\"Which of the above lines should be fixed?\",\n",
    "    [\n",
    "        '<span style=\"background-color: hsl(45.0, 100%, 80%)\">Line 3: `elif x < y`</span>',\n",
    "        '<span style=\"background-color: hsl(34.28571428571429, 100.0%, 80%)\">Line 5: `elif x < z`</span>',\n",
    "        '<span style=\"background-color: hsl(20.000000000000004, 100.0%, 80%)\">Line 6: `return y`</span>',\n",
    "        '<span style=\"background-color: hsl(120.0, 20.0%, 80%)\">Line 9: `return y`</span>',\n",
    "    ],\n",
    "     len(\" middle \\n\".strip()[:3])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, in the `middle()` example, the \"reddest\" line is also the one to be fixed.  Here is the fixed version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def middle_fixed(x, y, z):\n",
    "    if y < z:\n",
    "        if x < y:\n",
    "            return y\n",
    "        elif x < z:\n",
    "            return x\n",
    "    else:\n",
    "        if x > y:\n",
    "            return y\n",
    "        elif x > z:\n",
    "            return x\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "middle_fixed(2, 1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking Lines by Suspiciousness\n",
    "\n",
    "In a large program, there can be several locations (and events) that could be flagged as suspicious. It suffices that some large code block of say, 1,000 lines, is mostly executed in failing runs, and then all of this code block will be visualized in some shade of red. \n",
    "\n",
    "To further highlight the \"most suspicious\" events, one idea is to use a _ranking_ – that is, coming up with a list of events where those events most correlated with failures would be shown at the top. The programmer would then examine these events one by one and proceed down the list. We will show how this works for two \"correlation\" metrics – first the _Tarantula_ metric, as introduced above, and then the _Ochiai_ metric, which has shown to be one of the best \"ranking\" metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce a base class `RankingDebugger` with an abstract method `suspiciousness()` to be overloaded in subclasses. The method `rank_by_suspiciousness()` returns a list of all events observed, sorted by suspiciousness, highest first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankingDebugger(DifferenceDebugger):\n",
    "    def suspiciousness(self, event):\n",
    "        \"\"\"Return the suspiciousness (>=0) of an event. 0 stands for not suspicious.\"\"\"\n",
    "        return 0\n",
    "\n",
    "    def rank_by_suspiciousness(self):\n",
    "        \"\"\"Return a list of events, sorted by suspiciousness, highest first.\"\"\"\n",
    "        events = list(self.all_events())\n",
    "        events.sort(key=self.suspiciousness, reverse=True)\n",
    "        return events\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self.rank_by_suspiciousness())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Tarantula Metric\n",
    "\n",
    "We can use the Tarantula metric to sort lines according to their suspiciousness. The \"redder\" a line (a hue of 0.0), the more suspicious it is. We can simply define"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textit{suspiciousness}_\\textit{tarantula}(\\textit{event}) = 1 - \\textit{color hue}(\\textit{event})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\textit{color hue}$ is as defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce the `TarantulaDebugger` class, inheriting visualization capabilities from the `ContinuousSpectrumDebugger` class as well as the suspiciousness features from the `RankingDebugger` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TarantulaDebugger(ContinuousSpectrumDebugger, RankingDebugger):\n",
    "    def suspiciousness(self, event):\n",
    "        hue = self.hue(event)\n",
    "        if hue is None:\n",
    "            return None\n",
    "        return 1 - hue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us list `remove_html_markup()` with highlighted lines again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(TarantulaDebugger(CoverageCollector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.list_with_spectrum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's our ranking of lines, from most suspicious to least suspicious:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.rank_by_suspiciousness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.suspiciousness(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first line in the list is indeed the most suspicious; the two \"green\" lines come at the very end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the `middle()` function, we also obtain a ranking from \"reddest\" to \"greenest\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_middle(TarantulaDebugger(CoverageCollector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.list_with_spectrum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.rank_by_suspiciousness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.suspiciousness(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Ochiai Metric\n",
    "\n",
    "The _Ochiai_ Metric \\cite{Ochiai1957} first introduced in the biology domain \\cite{daSilvaMeyer2004} and later applied for fault localization by Abreu et al. \\cite{Abreu2009}, is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\textit{suspiciousness}_\\textit{ochiai} = \n",
    "\\frac\n",
    "{\\textit{failed}(\\textit{event})}\n",
    "{\\sqrt{\n",
    "\\bigl(\\textit{failed}(\\textit{event}) + \\textit{not-in-failed}(\\textit{event})\\bigr)\n",
    "\\times\n",
    "\\bigl(\\textit{failed}(\\textit{event}) + \\textit{passed}(\\textit{event})\\bigr)\n",
    "}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "\n",
    "* $\\textit{failed}(\\textit{event})$ is the number of times the event occurred in _failing_ runs\n",
    "* $\\textit{not-in-failed}(\\textit{event})$ is the number of times the event did _not_ occur in failing runs\n",
    "* $\\textit{passed}(\\textit{event})$ is the number of times the event occurred in _passing_ runs.\n",
    "\n",
    "We can easily implement this formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OchiaiDebugger(ContinuousSpectrumDebugger, RankingDebugger):\n",
    "    def suspiciousness(self, event):\n",
    "        failed = len(self.collectors_with_event(event, self.FAIL))\n",
    "        not_in_failed = len(self.collectors_without_event(event, self.FAIL))\n",
    "        passed = len(self.collectors_with_event(event, self.PASS))\n",
    "\n",
    "        try:\n",
    "            return failed / math.sqrt((failed + not_in_failed) * (failed + passed))\n",
    "        except ZeroDivisionError:\n",
    "            return None\n",
    "\n",
    "    def hue(self, event):\n",
    "        suspiciousness = self.suspiciousness(event)\n",
    "        if suspiciousness is None:\n",
    "            return None\n",
    "        return 1 - suspiciousness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applied on the `remove_html_markup()` function, the individual suspiciousness scores differ from Tarantula. However, we obtain a very similar visualization, and the same ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(OchiaiDebugger(CoverageCollector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.list_with_spectrum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.rank_by_suspiciousness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.suspiciousness(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same observations also apply for the `middle()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_middle(OchiaiDebugger(CoverageCollector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.list_with_spectrum(middle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.rank_by_suspiciousness()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.suspiciousness(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Useful is Ranking?\n",
    "\n",
    "So, which metric is better? The standard method to evaluate such rankings is to determine a _ground truth_ – that is, the set of locations that eventually are fixed – and to check at which point in the ranking any such location occurs – the earlier, the better. In our `remove_html_markup()` and `middle()` examples, both the Tarantula and the Ochiai metric perform flawlessly, as the \"culprit\" line is always ranked at the top. However, this need not always be the case; the exact performance depends on the nature of the code and the observed runs. (Also, the question of whether there always is exactly one possible location where the program can be fixed is open for discussion.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be surprised that over time, _several dozen_ metrics have been proposed \\cite{Wong2016}, each performing somewhat better or somewhat worse depending on which benchmark they were applied on. The two metrics discussed above each have their merits – the Tarantula metric was among the first such metrics, and the Ochiai metric is generally shown to be among the most effective ones \\cite{Abreu2009}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While rankings can be easily _evaluated_, it is not necessarily clear whether and how much they serve programmers. As stated above, the assumption of rankings is that developers examine one potentially defective statement after another until they find the actually defective one. However, in a series of human studies with developers, Parnin and Orso \\cite{Parnin2011} found that this assumption may not hold:\n",
    "\n",
    "> It is unclear whether developers can actually determine the faulty nature of a statement by simply looking at it, without any additional information (e.g., the state of the program when the statement was executed or the statements that were executed before or after that one).\n",
    "\n",
    "In their study, they found that rankings could help completing a task faster, but this effect was limited to experienced developers and simpler code. Artificially changing the rank of faulty statements had little to no effect, implying that developers would not strictly follow the ranked list of statements, but rather search through the code to understand it. At this point, a _visualization_ as in the Tarantula tool can be helpful to programmers as it _guides_ the search, but a _ranking_ that _defines_ where to search may be less useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having said that, ranking has its merits – notably as it comes to informing _automated_ debugging techniques. In the [chapter on program repair](Repair.ipynb), we will see how ranked lists of potentially faulty statements tell automated repair techniques where to try to repair the program first. And once such a repair is successful, we have a very strong indication on where and how the program could be fixed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Events besides Coverage\n",
    "\n",
    "We close this chapter with two directions for further thought. If you wondered why in the above code, we were mostly talking about `events` rather than lines covered, that is because our framework allows for tracking arbitrary events, not just coverage. In fact, any data item a collector can extract from the execution can be used for correlation analysis. (It may not be so easily visualized, though.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example. We define a `ValueCollector` class that collects pairs of (local) variables and their values during execution. Its `events()` method then returns the set of all these pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueCollector(Collector):\n",
    "    \"\"\"\"A class to collect local variables and their values.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.vars = set()\n",
    "\n",
    "    def collect(self, frame, event, arg):\n",
    "        local_vars = frame.f_locals\n",
    "        for var in local_vars:\n",
    "            value = local_vars[var]\n",
    "            self.vars.add(f\"{var} = {repr(value)}\")\n",
    "\n",
    "    def events(self):\n",
    "        \"\"\"A set of (variable, value) pairs observed\"\"\"\n",
    "        return self.vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply this collector on our set of HTML test cases, these are all the events that we obtain – essentially all variables and all values ever seen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(ContinuousSpectrumDebugger(ValueCollector))\n",
    "for event in debugger.all_events():\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, some of these events only occur in the failing run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for event in debugger.only_fail_events():\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these differences are spurious – the string `\"abc\"` (with quotes) only occurs in the failing run – but others, such as `quote` being True and `c` containing a single quote are actually relevant for explaining when the failure comes to be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even visualize the suspiciousness of the individual events, setting the (so far undiscussed) `color` flag for producing an event table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.event_table(color=True, args=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways one can continue from here.\n",
    "\n",
    "* Rather than checking for concrete values, one could check for more _abstract properties_, for instance – what is the sign of the value? What is the length of the string? \n",
    "* One could check for specifics of the _control flow_ – is the loop taken? How many times?\n",
    "* One could check for specifics of the _information flow_ – which values flow from one variable to another?\n",
    "\n",
    "There are lots of properties that all could be related to failures – and if we happen to check for the right one, we may obtain a much crisper definition of what causes the failure. We will come up with more ideas on properties to check as it comes to [mining specifications](SpecificationMining,ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Classifiers\n",
    "\n",
    "The metrics we have discussed so far are pretty _generic_ – that is, they are fixed no matter how the actual event space is structured. The field of _machine learning_ has come up with techniques that learn _classifiers_ from a given set of data – classifiers that are trained from labeled data and then can predict labels for new data sets. In our case, the labels are test outcomes (PASS and FAIL), whereas the data would be features of the events observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classifier by itself is not immediately useful for debugging (although it could predict whether future inputs will fail or not). Some classifiers, however, have great _diagnostic_ quality; that is, they can _explain_ how their classification comes to be. [Decision trees](https://scikit-learn.org/stable/modules/tree.html) fall into this very category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision tree contains a number of _nodes_, each one associated with a predicate. Depending on whether the predicate is true or false, we follow the given \"true\" or \"false\" branch to end up in the next node, which again contains a predicate. Eventually, we end up in the outcome predicted by the tree. The neat thing is that the node predicates actually give important hints on the circumstances that are _most relevant_ for deciding the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us illustrate this with an example. We build a class `ClassifyingDebugger` that trains a decision tree from the events collected. To this end, we need to set up our input data such that it can be fed into a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with identifying our _samples_ (runs) and the respective _labels_ (outcomes). All values have to be encoded into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyingDebugger(DifferenceDebugger):\n",
    "    \"\"\"A debugger implementing a decision tree for events\"\"\"\n",
    "\n",
    "    PASS_VALUE = +1\n",
    "    FAIL_VALUE = -1\n",
    "\n",
    "    def samples(self):\n",
    "        samples = {}\n",
    "        for collector in self.pass_collectors():\n",
    "            samples[collector.id()] = self.PASS_VALUE\n",
    "        for collector in debugger.fail_collectors():\n",
    "            samples[collector.id()] = self.FAIL_VALUE\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(ClassifyingDebugger(CoverageCollector))\n",
    "debugger.samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we identify the _features_, which in our case is the set of lines executed in each sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyingDebugger(ClassifyingDebugger):\n",
    "    def features(self):\n",
    "        features = {}\n",
    "        for collector in debugger.pass_collectors():\n",
    "            features[collector.id()] = collector.events()\n",
    "        for collector in debugger.fail_collectors():\n",
    "            features[collector.id()] = collector.events()\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(ClassifyingDebugger(CoverageCollector))\n",
    "debugger.features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All our features have names, which must be strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyingDebugger(ClassifyingDebugger):\n",
    "    def feature_names(self):\n",
    "        return [repr(feature) for feature in self.all_events()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(ClassifyingDebugger(CoverageCollector))\n",
    "debugger.feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the _shape_ for an individual sample, which is a value of +1 or -1 for each feature seen (i.e., +1 if the line was covered, -1 if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyingDebugger(ClassifyingDebugger):\n",
    "    def shape(self, sample):\n",
    "        x = []\n",
    "        features = self.features()\n",
    "        for f in self.all_events():\n",
    "            if f in features[sample]:\n",
    "                x += [+1]\n",
    "            else:\n",
    "                x += [-1]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(ClassifyingDebugger(CoverageCollector))\n",
    "debugger.shape(\"remove_html_markup(s='abc')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input X for the classifier now is a list of such shapes, one for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyingDebugger(ClassifyingDebugger):\n",
    "    def X(self):\n",
    "        X = []\n",
    "        samples = self.samples()\n",
    "        for key in samples:\n",
    "            X += [self.shape(key)]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(ClassifyingDebugger(CoverageCollector))\n",
    "debugger.X()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input Y for the classifier, in contrast, is the list of labels, again indexed by sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyingDebugger(ClassifyingDebugger):\n",
    "    def Y(self):\n",
    "        Y = []\n",
    "        samples = self.samples()\n",
    "        for key in samples:\n",
    "            Y += [samples[key]]\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(ClassifyingDebugger(CoverageCollector))\n",
    "debugger.Y()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all our data ready to be fit into a tree classifier. The method `classifier()` creates and returns the (tree) classifier for the observed runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_text, export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyingDebugger(ClassifyingDebugger):\n",
    "    def classifier(self):\n",
    "        classifier = DecisionTreeClassifier()\n",
    "        classifier = classifier.fit(self.X(), self.Y())\n",
    "        return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a special method to show classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifyingDebugger(ClassifyingDebugger):\n",
    "    def show_classifier(self, classifier):\n",
    "        dot_data = export_graphviz(classifier, out_file=None, \n",
    "                         filled=False, rounded=True,\n",
    "                         feature_names=self.feature_names(),\n",
    "                                class_names=[\"FAIL\", \"PASS\"],\n",
    "                                label='none',\n",
    "                                   node_ids=False,\n",
    "                                   impurity=False,\n",
    "                                   proportion=True,\n",
    "                         special_characters=True)\n",
    "\n",
    "        return graphviz.Source(dot_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the tree we get for our `remove_html_markup()` tests. The top predicate is whether the \"culprit\" line was executed (-1 means no, +1 means yes). If not (-1), the outcome is PASS. Otherwise, the outcome is TRUE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = test_debugger_html(ClassifyingDebugger(CoverageCollector))\n",
    "classifier = debugger.classifier()\n",
    "debugger.show_classifier(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even use our classifier to predict the outcome of additional runs. If, for instance, we execute all lines except for, say, Line 7, 9, and 11, our tree classifier would predict failure – because the \"culprit\" line 12 is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict([[1, 1, 1, 1, 1, 1, -1, 1, -1, 1, -1, 1, 1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, there are many ways to continue from here. Which events should we train the classifier from? How do classifiers compare in their performance and diagnostic quality? There are lots of possibilities left to explore, and we only begin to realize the potential for automated debugging.\n",
    "\n",
    "We will encounter decision trees again in the [chapter on determining failure circumstances](Alhazen.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synopsis\n",
    "\n",
    "This chapter introduces classes and techniques for _statistical debugging_ – that is, correlating specific events, such as lines covered, with passing and failing outcomes.\n",
    "\n",
    "To make use of the code in this chapter, use one of the provided `StatisticalDebugger` subclasses such as `TarantulaDebugger` or `OchiaiDebugger` instantiated with a `Collector` such as `CoverageCollector` denoting the type of events you want to correlate outcomes with. Here's a typical example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger = TarantulaDebugger(CoverageCollector)\n",
    "with debugger.collect_pass():\n",
    "    remove_html_markup(\"abc\")\n",
    "with debugger.collect_pass():\n",
    "    remove_html_markup('<b>abc</b>')\n",
    "with debugger.collect_fail():\n",
    "    remove_html_markup('\"abc\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then print out the observed events – in this case, line numbers – in a table, showing in which runs they occurred (`X`), and with colors highlighting the suspiciousness of the event. A \"red\" event means that the event predominantly occurs in failing runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.event_table(args=True, color=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you collected coverage with `CoverageCollector`, you can also visualize the code with similar colors, highlighting suspicious lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.list_with_spectrum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `rank_by_suspiciousness()` returns a ranked list of events, starting with the most suspicious. This is useful for automated techniques that need potential defect locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debugger.rank_by_suspiciousness()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are all classes defined in this chapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "from ClassDiagram import display_class_hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "display_class_hierarchy([TarantulaDebugger, OchiaiDebugger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "display_class_hierarchy([CoverageCollector, ValueCollector])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Lessons Learned\n",
    "\n",
    "* _Correlations_ between execution events and outcomes (pass/fail) can make important hints for debugging\n",
    "* Events occurring only (or mostly) during failing runs can be _highlighted_ and _ranked_ to guide the search\n",
    "* Important hints include whether the _execution of specific code locations_ correlates with failure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "Chapters that build on this one include\n",
    "\n",
    "* [how to determine invariants that correlate with failures](Invariants.ipynb)\n",
    "* [how to train learners on input features](Alhazen.ipynb)\n",
    "* [how to automatically repair programs](Repair.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The seminal works on statistical debugging are two papers:\n",
    "\n",
    "* \"Visualization of Test Information to Assist Fault Localization\" \\cite{Jones2002} by James Jones, Mary Jean Harrold, and John Stasko introducing Tarantula and its visualization. The paper won an ACM SIGSOFT 10-year impact award.\n",
    "* \"Bug Isolation via Remote Program Sampling\" by Ben Liblit, Alex Aiken, Alice X. Zheng, and Michael I. Jordan, introducing the term \"Statistical debugging\". Liblit won the ACM Doctoral Dissertation Award for this work.\n",
    "\n",
    "The Ochiai metric for fault localization was introduced by \\cite{Abreu2009}. The overview by Wong et al. \\cite{Wong2016} gives a comprehensive overview on the field of statistical fault localization.\n",
    "\n",
    "The study by Parnin and Orso \\cite{Parnin2011} is a must to understand the limitations of the technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Exercise 1: Statistical Dependencies\n",
    "\n",
    "Using the dependencies from [the chapter on slicing](Slicer.ipynb), can you determine which specific data or control dependencies correlate with failure?"
   ]
  }
 ],
 "metadata": {
  "ipub": {
   "bibliography": "fuzzingbook.bib",
   "toc": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
